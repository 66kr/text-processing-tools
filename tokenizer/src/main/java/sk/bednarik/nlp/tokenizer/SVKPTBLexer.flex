
package sk.bednarik.nlp.tokenizer;

// Stanford English Tokenizer -- a deterministic, fast, high-quality tokenizer.
// Copyright (c) 2002-2017 The Board of Trustees of
// The Leland Stanford Junior University. All Rights Reserved.
//
// This program is free software; you can redistribute it and/or
// modify it under the terms of the GNU General Public License
// as published by the Free Software Foundation; either version 2
// of the License, or (at your option) any later version.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.
//
// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.
//
// For more information, bug reports, fixes, contact:
//    Christopher Manning
//    Dept of Computer Science, Gates 2A
//    Stanford CA 94305-9020
//    USA
//    java-nlp-support@lists.stanford.edu
//    http://nlp.stanford.edu/software/


import java.io.Reader;
import java.util.logging.Logger;
import java.util.Map;
import java.util.Properties;
import java.util.Set;
import java.util.regex.Pattern;

import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.ling.CoreAnnotations;
import edu.stanford.nlp.process.Americanize;
import edu.stanford.nlp.process.CoreLabelTokenFactory;
import edu.stanford.nlp.process.LexedTokenFactory;
import edu.stanford.nlp.process.AbstractTokenizer;
import edu.stanford.nlp.process.LexerUtils;
import edu.stanford.nlp.util.StringUtils;
import edu.stanford.nlp.util.logging.Redwood;


/** Provides a tokenizer or lexer that does a pretty good job at
 *  deterministically tokenizing English according to Penn Treebank conventions.
 *  The class is a scanner generated by
 *  <a href="http://www.jflex.de/">JFlex</a> from the specification file
 *  {@code SVKPTBLexer.flex}.  As well as copying what is in the Treebank,
 *  it now contains many extensions to deal with modern text and encoding
 *  issues, such as recognizing URLs and common Unicode characters, and a
 *  variety of options for doing or suppressing certain normalizations.
 *  Although they shouldn't really be there, it also interprets certain of the
 *  characters between U+0080 and U+009F as Windows CP1252 characters, since many
 *  LDC corpora actually mix CP1252 content into supposedly utf-8 text.
 *  <p>
 *  <i>Fine points:</i> Output normalized tokens should not contain spaces,
 *  providing the normalizeSpace option is true.  The space will be turned
 *  into a non-breaking space (U+00A0). Otherwise, they can appear in
 *  a couple of token classes (phone numbers, fractions).
 *  The original
 *  PTB tokenization (messy) standard also escapes certain other characters,
 *  such as * and /, and normalizes things like " to `` or ''.  By default,
 *  this tokenizer does most of these things.  However, you can turn them
 *  off by using the ptb3Escaping=false option, or, parts of it on or off,
 *  or unicode character alternatives on with different options. Or you can turn
 *  everything on for strict Penn Treebank 3 tokenization. You can also build an
 *  invertible tokenizer, with which you can still access the original
 *  character sequence and the non-token whitespace around it in a CoreLabel.
 *  And you can ask for newlines to be tokenized.
 *  <p>
 *  <i>Character entities:</i> For legacy reasons, this file will parse and interpret
 *  some simple SGML/XML/HTML tags and character entities.  For modern formats
 *  like XML, you are better off doing XML parsing, and then running the
 *  tokenizer on text elements.  But, we and others frequently work with simple
 *  SGML text corpora that are not XML (like LDC text collections).  In practice,
 *  they only include very simple markup and a few simple entities, and the
 *  minimal character entity
 *  support in this file is enough to handle them. So we leave this functionality
 *  in, even though it could conceivably mess with a correct XML file if the
 *  output of decoding had things that look like character entities.  In general,
 *  handled symbols are changed to ASCII/Unicode forms, but handled accented
 *  letters are just left as character entities in words.
 *  <p>
 *  <i>Character support:</i> SVKPTBLexer works for a broad range of common Unicode
 *  characters. It recognizes all characters that are classed as letter (alphabetic)
 *  or digit in Unicode.
 *  It also matches all defined characters in the Unicode range U+0000-U+07FF
 *  excluding most control characters except the ones very standardly found in
 *  plain text documents. Finally, a fair range of other characters, such as many
 *  symbols commonly found in English Unicode text and emoji are also recognized.
 *  <p>
 *  <i>Implementation note:</i> The scanner is caseless, but note, if adding
 *  or changing regexps, that caseless does not extend inside character
 *  classes.  From the manual: "The %caseless option does not change the
 *  matched text and does not effect character classes. So [a] still only
 *  matches the character a and not A, too."  Note that some character
 *  classes deliberately don't have both cases, so the scanner's
 *  operation isn't completely case-independent, though it mostly is.
 *  <p>
 *  <i>Implementation note:</i> This Java class is automatically generated
 *  from SVKPTBLexer.flex using jflex.  DO NOT EDIT THE JAVA SOURCE.  This file
 *  has now been updated for JFlex 1.6.1+.
 *
 *  @author Tim Grow
 *  @author Christopher Manning
 *  @author Jenny Finkel
 */

%%

%class SVKPTBLexer
%unicode
%function next
%type Object
%char
%caseless
%state YyTokenizePerLine YyNotTokenizePerLine

%{

  /**
   * Constructs a new SVKPTBLexer.  You specify the type of result tokens with a
   * LexedTokenFactory, and can specify the treatment of tokens by boolean
   * options given in a comma separated String
   * (e.g., "invertible,normalizeParentheses=true").
   * If the String is {@code null} or empty, you get the traditional
   * PTB3 normalization behaviour (i.e., you get ptb3Escaping=false).  If you
   * want no normalization, then you should pass in the String
   * "ptb3Escaping=false".  See the documentation in the {@link PTBTokenizer}
   * class for full discussion of all the available options.
   *
   * @param r The Reader to tokenize text from
   * @param tf The LexedTokenFactory that will be invoked to convert
   *    each substring extracted by the lexer into some kind of Object
   *    (such as a Word or CoreLabel).
   * @param options Options to the tokenizer (see {@link PTBTokenizer})
   */
  public SVKPTBLexer(Reader r, LexedTokenFactory<?> tf, String options) {
    this(r);
    this.tokenFactory = tf;
    if (options == null) {
      options = "";
    }
    Properties prop = StringUtils.stringToProperties(options);
    Set<Map.Entry<Object,Object>> props = prop.entrySet();
    for (Map.Entry<Object,Object> item : props) {
      String key = (String) item.getKey();
      String value = (String) item.getValue();
      boolean val = Boolean.valueOf(value);
      if ("".equals(key)) {
        // allow an empty item
      } else if ("invertible".equals(key)) {
        invertible = val;
      } else if ("tokenizeNLs".equals(key)) {
        tokenizeNLs = val;
      } else if ("tokenizePerLine".equals(key)) {
        tokenizePerLine = val;
      } else if ("ptb3Escaping".equals(key)) {
        normalizeSpace = val;
        normalizeAmpersandEntity = val;
        // normalizeCurrency = val; // [cdm 2018]: We no longer do this as a default ptb3escaping
        normalizeFractions = val;
        normalizeParentheses = val;
        normalizeOtherBrackets = val;
        latexQuotes = val;
        unicodeQuotes = val;
        asciiQuotes = val;
        ptb3Ellipsis = val;
        unicodeEllipsis = val;
        ptb3Dashes = val;
      } else if ("americanize".equals(key)) {
        americanize = val;
      } else if ("normalizeSpace".equals(key)) {
        normalizeSpace = val;
      } else if ("normalizeAmpersandEntity".equals(key)) {
        normalizeAmpersandEntity = val;
      } else if ("normalizeCurrency".equals(key)) {
        normalizeCurrency = val;
      } else if ("normalizeFractions".equals(key)) {
        normalizeFractions = val;
      } else if ("normalizeParentheses".equals(key)) {
        normalizeParentheses = val;
      } else if ("normalizeOtherBrackets".equals(key)) {
        normalizeOtherBrackets = val;
      } else if ("latexQuotes".equals(key)) {
        latexQuotes = val;
      } else if ("unicodeQuotes".equals(key)) {
        unicodeQuotes = val;
        if (val) {
          latexQuotes = false; // need to override default
        }
      } else if ("asciiQuotes".equals(key)) {
        asciiQuotes = val;
        if (val) {
          latexQuotes = false; // need to override default
          unicodeQuotes = false;
        }
      } else if ("splitAssimilations".equals(key)) {
        splitAssimilations = val;
      } else if ("splitHyphenated".equals(key)) {
        splitHyphenated = val;
      } else if ("ptb3Ellipsis".equals(key)) {
        ptb3Ellipsis = val;
      } else if ("unicodeEllipsis".equals(key)) {
        unicodeEllipsis = val;
      } else if ("ptb3Dashes".equals(key)) {
        ptb3Dashes = val;
      } else if ("escapeForwardSlashAsterisk".equals(key)) {
        escapeForwardSlashAsterisk = val;
      } else if ("untokenizable".equals(key)) {
        switch (value) {
          case "noneDelete":
            untokenizable = UntokenizableOptions.NONE_DELETE;
            break;
          case "firstDelete":
            untokenizable = UntokenizableOptions.FIRST_DELETE;
            break;
          case "allDelete":
            untokenizable = UntokenizableOptions.ALL_DELETE;
            break;
          case "noneKeep":
            untokenizable = UntokenizableOptions.NONE_KEEP;
            break;
          case "firstKeep":
            untokenizable = UntokenizableOptions.FIRST_KEEP;
            break;
          case "allKeep":
            untokenizable = UntokenizableOptions.ALL_KEEP;
            break;
          default:
            throw new IllegalArgumentException("SVKPTBLexer: Invalid option value in constructor: " + key + ": " + value);
        }
      } else if ("strictTreebank3".equals(key)) {
        strictTreebank3 = val;
      } else {
        throw new IllegalArgumentException("SVKPTBLexer: Invalid options key in constructor: " + key);
      }
    }
    if (invertible) {
      if ( ! (tf instanceof CoreLabelTokenFactory)) {
        throw new IllegalArgumentException("SVKPTBLexer: the invertible option requires a CoreLabelTokenFactory");
      }
      prevWord = (CoreLabel) tf.makeToken("", 0, 0);
      prevWordAfter = new StringBuilder();
    }
    if (tokenizePerLine) {
      yybegin(YyTokenizePerLine);
    } else {
      yybegin(YyNotTokenizePerLine);
    }
  }


  /** Turn on to find out how things were tokenized. */
  private static final boolean DEBUG = false;

  /** A logger for this class */
  private static final Redwood.RedwoodChannels logger = Redwood.channels(SVKPTBLexer.class);

  private LexedTokenFactory<?> tokenFactory;
  private CoreLabel prevWord;
  private StringBuilder prevWordAfter;
  private boolean seenUntokenizableCharacter; // = false;
  private enum UntokenizableOptions { NONE_DELETE, FIRST_DELETE, ALL_DELETE, NONE_KEEP, FIRST_KEEP, ALL_KEEP }
  private UntokenizableOptions untokenizable = UntokenizableOptions.FIRST_DELETE;

  /* Flags begin with historical ptb3Escaping behavior. */
  private boolean invertible;
  private boolean tokenizeNLs;
  private boolean tokenizePerLine;
  private boolean americanize = false;
  private boolean normalizeSpace = true;
  private boolean normalizeAmpersandEntity = true;
  private boolean normalizeCurrency = false; // only $ and # in Penn Treebank 3 data, but we now allow other currency
  private boolean normalizeFractions = true;
  private boolean normalizeParentheses = true;
  private boolean normalizeOtherBrackets = true;
  private boolean latexQuotes = true;
  private boolean unicodeQuotes;
  private boolean asciiQuotes;
  private boolean ptb3Ellipsis = true;
  private boolean unicodeEllipsis;
  private boolean ptb3Dashes = true;
  private boolean escapeForwardSlashAsterisk = false; // this is true in Penn Treebank 3 but we don't do it now
  private boolean strictTreebank3 = false;
  private boolean splitAssimilations = true;
  private boolean splitHyphenated = false;

  /* Bracket characters and forward slash and asterisk:
   *
   * Original Treebank 3 WSJ
   * Uses -LRB- -RRB- as the representation for ( ) and -LCB- -RCB- as the representation for { }.
   * There are no occurrences of [ ], though there is some mention of -LSB- -RSB- in early documents.
   * There are no occurrences of < >.
   * All brackets are tagged -LRB- -RRB-  [This stays constant.]
   * Forward slash and asterisk are escaped by a preceding \ (as \/ and \*)
   *
   * Treebank 3 Brown corpus
   * Has -LRB- -RRB-
   * Has a few instances of unescaped [ ] in compounds (the token "A[fj]"
   * Neither forward slash or asterisk appears.
   *
   * Ontonotes (r4)
   * Uses -LRB- -RRB- -LCB- -RCB- -LSB- -RSB-.
   * Has a very few uses of < and > in longer tokens, which are not escaped.
   * Slash is not escaped. Asterisk is not escaped.
   *
   * LDC2012T13-eng_web_tbk (Google web treebank)
   * Has -LRB- -RRB-
   * Has { and } used unescaped, treated as brackets.
   * Has < and > used unescaped, sometimes treated as brackets.  Sometimes << and >> are treated as brackets!
   * Has [ and ] used unescaped, treated as brackets.
   * Slash is not escaped. Asterisk is not escaped.
   *
   * Reasonable conclusions for now:
   * - Never escape < >
   * - Still by default escape [ ] { } but it can be turned off.  Use -LSB- -RSB- -LCB- -RCB-.
   * Move to not escaping slash and asterisk, and delete escaping in PennTreeReader.
   */

  public static final String openparen = "-LRB-";
  public static final String closeparen = "-RRB-";
  public static final String openbrace = "-LCB-";
  public static final String closebrace = "-RCB-";
  public static final String ptbmdash = "--";
  public static final String ptb3EllipsisStr = "...";
  public static final String unicodeEllipsisStr = "\u2026";

  /* This pattern now also include newlines, since we sometimes allow them in SGML tokens.... */
  private static final Pattern SINGLE_SPACE_PATTERN = Pattern.compile("[ \r\n]");
  private static final Pattern LEFT_PAREN_PATTERN = Pattern.compile("\\(");
  private static final Pattern RIGHT_PAREN_PATTERN = Pattern.compile("\\)");


  /* -- upto (2017) -- */

  private void breakByHyphens(String in) {
    if (splitHyphenated) {
      int firstHyphen = in.indexOf('-');
      yypushback(in.length() - firstHyphen);
    }
  }

  /**
   * If an apparent negative number is generated from a hyphenated word, tokenize the hyphen.
   */
  private void handleHyphenatedNumber(String in) {
    // Strip dashes from hyphenated words
    if (prevWord != null && in.length() >= 2 && in.charAt(0) == '-' && in.charAt(1) != '-') {
      String lastWord = prevWord.originalText();
      switch (lastWord) {
        case "mid":
        case "late":
        case "early":
          yypushback(in.length() - 1);
        default:
          if (lastWord.length() > 0 &&
              lastWord.charAt(0) <= 57 && lastWord.charAt(0) >= 48 &&
              prevWordAfter != null && prevWordAfter.length() == 0) {  // last word is a number as well
            yypushback(in.length() - 1);
          }
          break;
      }
    }
  }


  private static String removeFromNumber(String in) {
    StringBuilder out = null;
    if ("-".equals(in)) {
      // Shortcut for if we split on hyphens
      return in;
    }

    // \u00AD is the soft hyphen character, which we remove, regarding it as inserted only for line-breaking
    // \u066C\u2009\u202F are thousands separator characters that it seems safe to remove.
    int length = in.length();
    for (int i = 0; i < length; i++) {
      char ch = in.charAt(i);
      if (ch == '\u00AD' || ch == '\u066C' || ch == '\u2009' || ch == '\u202F') {
        if (out == null) {
          out = new StringBuilder(length);
          out.append(in.substring(0, i));
        }
      } else if (out != null) {
        out.append(ch);
      }
    }
    if (out == null) {
      return in;
    }
    return out.toString().trim();
  }


  /*
   * This class has now been extended to cover the main Windows CP1252 characters,
   * at either their correct Unicode codepoints, or in their invalid
   * positions as 8 bit chars inside the iso-8859 control region.
   *
   * ellipsis   85      0133    2026    8230   COMPLICATED!! Also a newline character for IBM 390; we let ellipsis win
   * dagger     86    2020
   * double dagger 87 2021
   * single quote curly starting        91      0145    2018    8216
   * single quote curly ending  92      0146    2019    8217
   * double quote curly starting        93      0147    201C    8220
   * double quote curly ending  94      0148    201D    8221
   * bullet     95
   * en dash    96      0150    2013    8211
   * em dash    97      0151    2014    8212
   */

  private static final Pattern singleQuote = Pattern.compile("&apos;|'");
  // If they typed `` they probably meant it, but if it's '' or mixed, we use our heuristics.
  private static final Pattern doubleQuote = Pattern.compile("\"|''|'`|`'|&quot;");

  // 82,84,91,92,93,94 aren't valid unicode points, but sometimes they show
  // up from cp1252 and need to be translated
  private static final Pattern leftSingleQuote = Pattern.compile("[\u0082\u008B\u0091\u2018\u201A\u201B\u2039]");
  private static final Pattern rightSingleQuote = Pattern.compile("[\u0092\u009B\u00B4\u2019\u203A]");
  private static final Pattern leftDoubleQuote = Pattern.compile("[\u0084\u0093\u201C\u201E\u00AB]|[\u0091\u2018]'");
  private static final Pattern rightDoubleQuote = Pattern.compile("[\u0094\u201D\u00BB]|[\u0092\u2019]'");

  private static String latexQuotes(String in, boolean probablyLeft) {
    // System.err.println("Handling quote on " + in + " probablyLeft=" + probablyLeft);
    String s1 = in;
    if (probablyLeft) {
      s1 = singleQuote.matcher(s1).replaceAll("`");
      s1 = doubleQuote.matcher(s1).replaceAll("``");
    } else {
      s1 = singleQuote.matcher(s1).replaceAll("'");
      s1 = doubleQuote.matcher(s1).replaceAll("''");
    }
    s1 = leftSingleQuote.matcher(s1).replaceAll("`");
    s1 = rightSingleQuote.matcher(s1).replaceAll("'");
    s1 = leftDoubleQuote.matcher(s1).replaceAll("``");
    s1 = rightDoubleQuote.matcher(s1).replaceAll("''");
    // System.err.println("  Mapped to " + s1);
    return s1;
  }

  // U+00B4 should be acute accent, but stuff happens
  private static final Pattern asciiSingleQuote = Pattern.compile("&apos;|[\u0082\u008B\u0091\u00B4\u2018\u0092\u2019\u009B\u201A\u201B\u2039\u203A']");
  private static final Pattern asciiDoubleQuote = Pattern.compile("&quot;|[\u0084\u0093\u201C\u0094\u201D\u201E\u00AB\u00BB\"]");

  private static String asciiQuotes(String in) {
    String s1 = in;
    s1 = asciiSingleQuote.matcher(s1).replaceAll("'");
    s1 = asciiDoubleQuote.matcher(s1).replaceAll("\"");
    return s1;
  }
